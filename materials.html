<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Materials - Text Watermarking Survey</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            display: flex;
            flex-direction: column;
            align-items: center;
            background-color: #f0f4f8;
            color: #333;
        }
        header {
            background: linear-gradient(135deg, #2c3e50, #34495e);
            color: #fff;
            padding: 20px 0;
            text-align: center;
            width: 100%;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        nav {
            background: #34495e;
            color: #fff;
            padding: 15px;
            text-align: center;
            width: 100%;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        nav a {
            color: #fff;
            margin: 0 15px;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s ease;
        }
        nav a:hover {
            color: #3498db;
        }
        section {
            padding: 40px;
            margin-bottom: 80px;
            max-width: 1000px;
            width: 90%;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        footer {
            background: linear-gradient(135deg, #2c3e50, #34495e);
            color: #fff;
            text-align: center;
            padding: 15px 0;
            position: fixed;
            width: 100%;
            bottom: 0;
            box-shadow: 0 -2px 5px rgba(0,0,0,0.1);
        }
        .pdf-link {
            display: inline-block;
            background-color: #3498db;
            color: white;
            padding: 12px 25px;
            text-decoration: none;
            border-radius: 25px;
            margin-top: 30px;
            transition: background-color 0.3s ease;
        }
        .pdf-link:hover {
            background-color: #2980b9;
        }
        .citation {
            margin-top: 40px;
            margin-bottom: 80px;
            position: relative;
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.05);
        }
        .citation pre {
            background: #fff;
            border: 1px solid #e0e0e0;
            border-left: 5px solid #3498db;
            page-break-inside: avoid;
            font-family: 'Courier New', Courier, monospace;
            font-size: 14px;
            line-height: 1.6;
            margin-bottom: 1.6em;
            max-width: 100%;
            overflow: auto;
            padding: 1em 1.5em;
            display: block;
            word-wrap: break-word;
        }
        .copy-btn {
            display: inline-block;
            background-color: #3498db;
            color: white;
            padding: 10px 20px;
            text-decoration: none;
            border-radius: 25px;
            margin-top: 10px;
            cursor: pointer;
            position: absolute;
            top: 3.5em;
            right: 1em;
            opacity: 0;
            transition: opacity 0.3s, background-color 0.3s ease;
        }
        .citation:hover .copy-btn {
            opacity: 1;
        }
        .copy-btn:hover {
            background-color: #2980b9;
        }
        .section-content {
            margin-left: 20px;
        }
        a {
            color: #3498db;
            text-decoration: none;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        h2, h3, h4 {
            color: #2c3e50;
        }
        ul {
            padding-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
    </style>
</head>
<body>

<header>
    <h1>Text Watermarking in the Era of Large Language Models</h1>
</header>

<nav>
    <a href="index.html">Home</a>
    <a href="news.html">News</a>
    <a href="people.html">People</a>
    <a href="#materials">Materials</a>
</nav>

<section id="materials">
    <h2>Text Watermarking Related Papers</h2>
    
    <h3>Watermarking for Existing Text</h3>
    <div class="section-content">
        <h4>Format-based Watermarking</h4>
        <ul>
            <li><a href="https://www.researchgate.net/publication/3233597_Electronic_Marking_and_Identification_Techniques_to_Discourage_Document_Copying">Electronic marking and identification techniques to discourage document copying.</a></li>
            <li><a href="https://www.researchgate.net/publication/256991692_UniSpaCh_A_text-based_data_hiding_method_using_Unicode_space_characters">UniSpaCh: A text-based data hiding method using Unicode space characters.</a></li>
            <li><a href="https://dl.acm.org/doi/pdf/10.1145/2938503.2938510">Content-preserving text watermarking through unicode homoglyph substitution.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.08920">Embarrassingly simple text watermarks.</a></li>
        </ul>
        
        <h4>Lexical-based Watermarking</h4>
        <ul>
            <li><a href="https://arxiv.org/pdf/2305.05773">Deeptextmark: Deep learning based text watermarking for detection of large language model generated text.</a></li>
            <li><a href="https://dl.acm.org/doi/10.1145/1161366.1161397">The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions.</a></li>
            <li><a href="https://arxiv.org/pdf/2305.08883">Watermarking text generated by black-box language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2305.01904">Robust multi-bit natural language watermarking through invariant features.</a></li>
        </ul>
        
        <h4>Syntactic-based Watermarking</h4>
        <ul>
            <li><a href="https://link.springer.com/chapter/10.1007/3-540-45496-9_14">Natural language watermarking: Design, analysis, and a proof-of-concept implementation.</a></li>
            <li><a href="https://www.researchgate.net/publication/348677551_Natural_language_watermarking_via_morphosyntactic_alterations">Natural language watermarking via morphosyntactic alterations.</a></li>
            <li><a href="https://dl.acm.org/doi/10.1145/1178766.1178777">Words are not enough: sentence level natural language watermarking.</a></li>
        </ul>
        
        <h4>Generation-based Watermarking</h4>
        <ul>
            <li><a href="https://arxiv.org/pdf/2009.03015">Adversarial watermarking transformer: Towards tracing text provenance with data hiding.</a></li>
            <li><a href="https://arxiv.org/pdf/2407.04411">Waterfall: Framework for Robust and Scalable Text Watermarking.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.12362">REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2406.14517">PostMark: A Robust Blackbox Watermark for Large Language Models. (Future)</a></li>
        </ul>
    </div>
    
    <h3>Watermarking for LLMs</h3>
    <div class="section-content">
        <h4>Watermarking during Logits Generation</h4>
        <ul>
            <li><a href="https://arxiv.org/pdf/2301.10226">A watermark for large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2305.15060">Who Wrote this Code? Watermarking for Code Generation.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.10669">Unbiased Watermark for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.07710">DiPmark: A Stealthy, Efficient and Resilient Watermark for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2308.00221">Advancing Beyond Identification: Multi-bit Watermark for Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2306.17439">Provable Robust Watermarking for AI-Generated Text.</a></li>
            <li><a href="https://arxiv.org/pdf/2307.15992">Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs.</a></li>
            <li><a href="https://arxiv.org/pdf/2306.04634">On the Reliability of Watermarks for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.06356">A Semantic Invariant Robust Watermark for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2402.14007">Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2307.16230">An Unforgeable Publicly Verifiable Watermark for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2308.00113">Three bricks to consolidate watermarks for large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.18491">Publicly Detectable Watermarking for Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2311.08721">A Robust Semantics-based Watermark for Large Language Model against Paraphrasing.</a></li>
            <li><a href="https://arxiv.org/pdf/2403.13485">An Entropy-based Text Watermarking Detection Method.</a></li>
            <li><a href="https://arxiv.org/pdf/2307.13808">Watermarking conditional text generation for ai detection: Unveiling challenges and a semantic-aware watermark remedy.</a></li>
            <li><a href="https://arxiv.org/pdf/2404.15639">Codeip: A grammar-guided multi-bit watermark for large language models of code.</a></li>
            <li><a href="https://arxiv.org/pdf/2404.15639">Adaptive Text Watermark for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2312.17295">Optimizing watermarks for large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2311.09832">WatME: Towards Lossless Watermarking Through Lexical Redundancy.</a></li>
            <li><a href="https://arxiv.org/pdf/2405.08400">Stylometric Watermarks for Large Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2404.02138">Topic-Based Watermarks for LLM-Generated Text. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2406.02603">Distortion-free Watermarks are not Truly Distortion-free under Watermark Key Collisions. (Future)</a></li>
            <li><a href="https://openreview.net/pdf?id=6FXotcWvO2">A Nested Watermark for Large Language Models. (Future)</a></li>
            <li><a href="https://openreview.net/forum?id=JmNvS3Z4RF">Balanced Watermark: A Simple High-Imperceptibility Watermark for Large Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/abs/2310.00833">Necessary and Sufficient Watermark for Large Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2408.01354">MCGMark: An Encodable and Robust Online Watermark for LLM-Generated Malicious Code. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2407.13803">Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2405.11109">Watermarking Language Models for Many Adaptive Users. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2402.12948">GumbelSoft: Diversified Language Model  Watermarking via the GumbelMax-trick (Future)</a></li>
        </ul>
        
        <h4>Watermarking during Sampling</h4>
        <ul>
            <li><a href="https://arxiv.org/pdf/2306.09194">Undetectable watermarks for language models.</a></li>
            <li><a href="https://www.scottaaronson.com/talks/watermark.ppt">Watermarking GPT outputs.</a></li>
            <li><a href="https://arxiv.org/pdf/2307.15593">Robust Distortion-free Watermark for Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2310.03991">SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation.</a></li>
            <li><a href="https://arxiv.org/pdf/2402.11399">k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text.</a></li>
            <li><a href="https://arxiv.org/pdf/2405.14604">A Watermark for Low-entropy and Unbiased Generation in Large Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2406.01946">Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2406.02633">Edit Distance Robust Watermarks for Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2404.01245">A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2410.02099">A Watermark for Black-Box Language Models. (Future)</a></li>
        </ul>
        
        <h4>Watermarking during LLM Training</h4>
        <ul>
            <li><a href="https://arxiv.org/pdf/2308.14401">CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2110.12925">Coprotector: Protect open-source code against unauthorized training usage with data poisoning.</a></li>
            <li><a href="https://arxiv.org/pdf/2312.04469">On the Learnability of Watermarks for Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2403.05842">Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance.</a></li>
            <li><a href="https://arxiv.org/pdf/2403.10553">Learning to watermark llm-generated text via reinforcement learning.</a></li>
            <li><a href="https://arxiv.org/pdf/2402.10892">Proving membership in LLM pretraining data via data watermarks. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2402.14904">Watermarking Makes Language Models Radioactive. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2407.17417">Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data. (Future)</a></li>
        </ul>
    </div>
    
    <h3>Watermark Attacks</h3>
    <div class="section-content">
        <ul>
            <li><a href="https://arxiv.org/pdf/2402.14007">Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2403.17983">Is Watermarking LLM-Generated Code Robust?</a></li>
            <li><a href="https://arxiv.org/pdf/2403.10020">Lost in Overlap: Exploring Watermark Collision in LLMs.</a></li>
            <li><a href="https://arxiv.org/pdf/2303.11156">Can AI-Generated Text be Reliably Detected?</a></li>
            <li><a href="https://arxiv.org/pdf/2403.14719">Bypassing LLM Watermarks with Color-Aware Substitutions.</a></li>
            <li><a href="https://arxiv.org/pdf/2405.19677">Large Language Model Watermark Stealing With Mixed Integer Programming.</a></li>
            <li><a href="https://arxiv.org/pdf/2402.19361">Watermark stealing in large language models.</a></li>
            <li><a href="https://arxiv.org/pdf/2311.04378">Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2312.04469">On the Learnability of Watermarks for Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2402.16187">No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2407.14206">Watermark Smoothing Attacks against Language Models. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2410.02693">Discovering Clues of Spoofed LM Watermarks (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2410.02440">Optimizing Adaptive Attacks against Content Watermarks for Language Models (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2410.03168">Can Watermarked LLMs be Identified by Users via Crafted Prompts? (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2410.03168">Black-Box Detection of Language Model Watermarks (Future)</a></li>
        </ul>
    </div>

    <h3>Benchmarks and Tools</h3>
    <div class="section-content">
        <ul>
            <li><a href="https://arxiv.org/pdf/2311.07138">WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2403.19548">WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models.</a></li>
            <li><a href="https://arxiv.org/pdf/2312.00273">Mark My Words: Analyzing and Evaluating Language Model Watermarks.</a></li>
            <li><a href="https://arxiv.org/pdf/2405.10051">MarkLLM: An Open-Source Toolkit for LLM Watermarking.</a></li>
            <li><a href="https://arxiv.org/pdf/2406.03728">Evaluating Durability: Benchmark Insights into Multimodal Watermarking. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2312.02382">New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking. (Future)</a></li>
            <li><a href="https://arxiv.org/pdf/2407.04794">On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks. (Future)</a></li>
        </ul>
    </div>
        
</section>

<footer>
    <p>© 2024 Text Watermarking Survey</p>
</footer>

</body>
</html>